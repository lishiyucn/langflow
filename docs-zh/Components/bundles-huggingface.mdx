---
title: Hugging Face
slug: /bundles-huggingface
---

import Icon from "@site/src/components/icon";

[Bundle组件包](/components-bundle-components) 包含支持 Langflow 特定第三方集成的自定义组件。

**Hugging Face** bundle 中的组件需要访问 Hugging Face API。

有关 Hugging Face 组件使用的 Hugging Face 功能和特性的更多信息，请参阅 [Hugging Face 文档](https://huggingface.co/docs)。

## Hugging Face 文本生成

**Hugging Face** 组件通过向 Hugging Face API 发送请求来使用指定模型生成文本，该 API 是托管在 Hugging Face 上的模型的托管推理 API。
需要身份验证。

该组件可以输出 **模型响应** ([`Message`](/data-types#message)) 或 **语言模型** ([`LanguageModel`](/data-types#languagemodel))。
具体来说，**语言模型** 输出是根据组件参数配置的 [`HuggingFaceHub`](https://python.langchain.com/docs/integrations/providers/huggingface/) 实例。

当您想要使用 Hugging Face 模型作为其他 LLM 驱动组件（如 **Language Model** 或 **Smart Function** 组件）的 LLM 时，请使用 **语言模型** 输出。

更多信息请参阅 [**语言模型** 组件](/components-models)。

### Hugging Face 文本生成参数

在可视化编辑器中，许多 **Hugging Face** 组件输入参数默认是隐藏的。
您可以通过 [组件标题菜单](/concepts-components#component-menus) 中的 <Icon name="SlidersHorizontal" aria-hidden="true"/> **控制** 来切换参数。

| 名称 | 类型 | 描述 |
|------|------|-------------|
| model_id | String | 输入参数。来自 Hugging Face Hub 的模型 ID。例如，"gpt2"、"facebook/bart-large"。 |
| huggingfacehub_api_token | SecretString | 输入参数。您用于身份验证的 [Hugging Face API 令牌](https://huggingface.co/docs/hub/security-tokens)。 |
| temperature | Float | 输入参数。控制输出中的随机性。范围：[0.0, 1.0]。默认：0.7。 |
| max_new_tokens | Integer | 输入参数。要生成的最大令牌数。默认：512。 |
| top_p | Float | 输入参数。核采样参数。范围：[0.0, 1.0]。默认：0.95。 |
| top_k | Integer | 输入参数。Top-k 采样参数。默认：50。 |
| model_kwargs | Dictionary | 输入参数。传递给模型的额外关键字参数。 |

## Hugging Face Embeddings Inference

使用 **Hugging Face Embeddings Inference** 组件来创建使用 Hugging Face 托管模型或您自己本地托管模型的嵌入。

该组件使用 [Hugging Face Inference API 模型](https://huggingface.co/models) 生成嵌入。
当不使用本地模型时需要身份验证。

有关在流程中使用嵌入模型组件的更多信息，请参阅 [**嵌入模型** 组件](/components-embedding-models) 和 [使用本地 Hugging Face 嵌入模型](#local-hugging-face-model)。

### Hugging Face Embeddings Inference 参数

| 名称 | 显示名称 | 信息 |
|------|--------------|------|
| API Key | API Key | 输入参数。如果需要，您用于访问 Hugging Face Inference API 的 [Hugging Face API 令牌](https://huggingface.co/docs/hub/security-tokens)。本地推理模型不需要 API 密钥。 |
| API URL | API URL | 输入参数。Hugging Face Inference API 的 URL。 |
| Model Name | Model Name | 输入参数。用于嵌入的模型名称。 |

### 使用本地 Hugging Face 嵌入模型 {#local-hugging-face-model}

要将本地 Hugging Face 模型连接到 **Hugging Face Embeddings Inference** 组件并在流程中使用它，请按照以下步骤操作：

1. 运行 [本地 Hugging Face 嵌入推理](https://huggingface.co/docs/text-embeddings-inference/local_cpu)。

2. 在此示例中，从 [**Vector Store RAG** 模板](/vector-store-rag) 创建一个流程。

3. 将两个 **OpenAI Embeddings** 组件替换为 **Hugging Face Embeddings Inference** 组件。

    确保将每个嵌入模型组件的 **Embedding Model** 端口重新连接到其对应的 **Astra DB** 向量存储组件。

4. 配置 **Astra DB** 向量存储组件以连接到您的 Astra 组织，或者将两个 **Astra DB** 向量存储组件替换为其他 [向量存储组件](/components-vector-stores)。

5. 将每个 **Hugging Face Embeddings Inference** 组件连接到您的本地推理模型：

    * **Inference Endpoint**：输入您的本地推理模型的 URL。
    * **API Key**：对于本地推理可以为空。
    * **Model Name**：如果未自动检测到，请输入您的本地推理模型的名称。

6. 要测试流程，请点击 **Playground**，然后输入一些文本以生成嵌入。