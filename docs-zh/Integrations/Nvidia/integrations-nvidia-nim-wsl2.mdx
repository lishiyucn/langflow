---
title:  Integrate NVIDIA NIMs with Langflow
slug: /integrations-nvidia-ingest-wsl2
---

在安装了 [Windows Subsystem for Linux 2 (WSL2)](https://learn.microsoft.com/en-us/windows/wsl/install) 的 RTX Windows 系统上连接 **Langflow** 与 **NVIDIA NIM**。

[NVIDIA NIM (NVIDIA Inference Microservices)](https://docs.nvidia.com/nim/index.html) 提供容器来自托管 GPU 加速推理微服务。
在此示例中，您将 **Langflow** 中的模型组件连接到在带有 **WSL2** 的 **RTX Windows 系统**上部署的 `mistral-nemo-12b-instruct` NIM。

有关 NVIDIA NIM 的更多信息，请参阅 [NVIDIA 文档](https://docs.nvidia.com/nim/index.html)。

## 先决条件

* [已安装 NVIDIA NIM WSL2](https://docs.nvidia.com/nim/wsl2/latest/getting-started.html)
* 根据模型指令部署的 NIM 容器。不同模型的先决条件不同。
例如，要部署 `mistral-nemo-12b-instruct` NIM，请按照[您模型的部署概览](https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct/deploy?environment=wsl2.md)中 **Windows on RTX AI PCs (Beta)** 的指令进行操作
* Windows 11 build 23H2 或更高版本
* 至少 12 GB RAM

## 在流中使用 NVIDIA NIM

要将您部署的 NIM 与 Langflow 连接，请将 **NVIDIA** 模型组件添加到流中。

1. 创建一个[基本提示流](/get-started-quickstart)。
2. 用 **NVIDIA** 组件替换 **OpenAI** 模型组件。
3. 在 **NVIDIA** 组件的 **Base URL** 字段中，添加您的 NIM 可访问的 URL。如果您按照您模型的[部署指令](https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct/deploy?environment=wsl2.md)进行操作，则值为 `http://localhost:8000/v1`。
4. 在 **NVIDIA** 组件的 **NVIDIA API Key** 字段中，添加您的 NVIDIA API 密钥。
5. 从 **Model Name** 下拉菜单中选择您的模型。
6. 打开 **Playground** 并与您的 **NIM** 模型聊天。